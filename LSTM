import torch
import torch.nn as nn
from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence


class LSTMModel(nn.Module):
    """
    LSTM classifier 

    Assumptions:
    - Input x: (batch, seq_len, input_size)
    - Padded timesteps are filled with NaNs (usually at the end)
    """

    def __init__(
        self,
        input_size: int,
        hidden_size: int,
        num_layers: int,
        num_classes: int,
        lstm_dropout: float = 0.4,
        input_dropout: float = 0.3,
    ):
        super().__init__()

        self.input_dropout = nn.Dropout(input_dropout)

        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=lstm_dropout if num_layers > 1 else 0.0,
        )

        self.fc = nn.Linear(hidden_size, num_classes)

    @staticmethod
    def _lengths_from_nan_padding(x: torch.Tensor) -> torch.Tensor:
        valid = ~torch.any(torch.isnan(x), dim=-1)     # (B, T)
        lengths = valid.sum(dim=1)                    # (B,)
        lengths = torch.clamp(lengths, min=1)
        return lengths.to(dtype=torch.long, device="cpu")

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Returns:
            logits: (batch, num_classes)
        """
        if x.dim() != 3:
            raise ValueError(f"Expected x to have shape (batch, seq, feat), got {tuple(x.shape)}")

        x = x.float()
        x = self.input_dropout(x)

        lengths = self._lengths_from_nan_padding(x)

        # Replace NaNs with zeros; packing will ignore padded steps via lengths.
        x_clean = torch.nan_to_num(x, nan=0.0)

        packed = pack_padded_sequence(
            x_clean, lengths=lengths, batch_first=True, enforce_sorted=False
        )
        packed_out, _ = self.lstm(packed)

        out, _ = pad_packed_sequence(packed_out, batch_first=True)  # (B, T_max, H)
        B, T, H = out.shape

        # last valid hidden state per sample
        idx = (lengths.to(out.device) - 1).view(B, 1, 1).expand(B, 1, H)
        last_h = out.gather(dim=1, index=idx).squeeze(1)            # (B, H)

        logits = self.fc(last_h)                                     # (B, C)
        return logits
