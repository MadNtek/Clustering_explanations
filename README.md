# Clustering_explanations
Attention mechanisms for explaining time-series clustering

ğŸ“Œ Models Overview
1ï¸âƒ£ LSTM (Baseline)

A standard LSTM classifier that processes variable-length time-series to predict the number of clusters where each individual belongs (clustering-independent output)

Output: class logits

2ï¸âƒ£ LSTM + Temporal Attention

An LSTM followed by an additive attention mechanism over time steps:

Input â†’ LSTM â†’ Temporal Attention â†’ Classifier


This allows the model to focus on the most informative time steps and improve interpretability via attention weights

Outputs: class logits

3ï¸âƒ£ Feature + Temporal Attention (Multi-Head Attention)

A dual-attention model that applies:

- Feature attention (across variables)

- Temporal attention (across time steps)

Input
 â”œâ”€ Feature Attention
 â”œâ”€ Temporal Attention
 â””â”€ Concatenation â†’ Classifier


Outputs: class logits, feature attention weights, temporal attention weights

ğŸ—‚ Repository Structure
.
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ LSTM.py
â”‚   â”œâ”€â”€ LSTM_att.py
â”‚   â”œâ”€â”€ LSTM_att_training.py
â”‚   â”œâ”€â”€ FeatTempAttClassifier.py
â”‚   â””â”€â”€ FeatTempAttClassifier_training.py
â”‚   â””â”€â”€ TempAtt_vizualization.py
â”‚   â””â”€â”€ FeatTempAtt_vizualization.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

ğŸ“¥ Input Format

All models expect input tensors of shape:

(batch_size, time_steps, num_features)


