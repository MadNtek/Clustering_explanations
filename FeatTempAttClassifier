import torch
import torch.nn as nn

class FeatTempAttClassifier(nn.Module):
    def __init__(self, input_dim, output_dim, dropout_prob=0.4):
        """
        Initializes the feature temporal attention classifier.

        Args:
            input_dim (int): Number of input features.
            output_dim (int): Number of output classes for classification.
            dropout_prob (float): Dropout probability.
        """
        super(FeatTempAttClassifier, self).__init__()

        # Multihead Attention layers for temporal and feature attention
        self.multihead_attn0 = nn.MultiheadAttention(embed_dim=153, num_heads=1, batch_first=True)
        self.multihead_attn1 = nn.MultiheadAttention(embed_dim=input_dim, num_heads=1, batch_first=True)

        # Dropout layer for regularization
        self.dropout = nn.Dropout(p=dropout_prob)

        # Final fully connected layer for classification
        self.fc = nn.Linear(2 * input_dim, output_dim)

        # Linear transformations for attention layers
        self.query = nn.Linear(input_dim, input_dim)
        self.key = nn.Linear(input_dim, input_dim)
        self.value = nn.Linear(input_dim, input_dim)

    def forward(self, x, lengths, mask=None):
        """
        Forward pass of the model.

        Args:
            x (Tensor): Input tensor of shape (batch_size, num_time_steps, num_features).
            lengths (List[int]): List of sequence lengths for each batch.
            mask (Tensor, optional): Mask for padding tokens, defaults to None.

        Returns:
            output (Tensor): Output tensor for classification.
            att1 (Tensor): Attention weights from feature attention.
            att2 (Tensor): Attention weights from temporal attention.
        """
        batch_size, num_time_steps, num_features = x.shape

        # Replace NaN values with 0 in input
        X_filled = torch.nan_to_num(x.float(), nan=0.0)

        # Apply feature attention (multihead attention over features)
        attn_output, att2 = self.multihead_attn1(X_filled, X_filled, X_filled, key_padding_mask=mask)

        # Apply temporal attention (multihead attention over time steps)
        context = X_filled.transpose(1, 2)
        attn_output1, att1 = self.multihead_attn0(context, context, context, key_padding_mask=None)

        # Reformat context back for concatenation
        context = attn_output1.transpose(1, 2).contiguous().view(x.shape)

        # Extract the last valid output for each sequence (based on lengths)
        last_outputs = torch.stack([attn_output[i, length - 1] for i, length in enumerate(lengths)])
        last_outputs2 = torch.stack([context[i, length - 1] for i, length in enumerate(lengths)])

        # Concatenate the outputs from both attention mechanisms
        outputs2 = torch.cat([last_outputs, last_outputs2], 1)

        # Apply dropout for regularization
        outputs2 = self.dropout(outputs2)

        # Final classification layer
        output = self.fc(outputs2)

        return output, att1, att2
