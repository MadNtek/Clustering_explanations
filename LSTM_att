import torch.nn as nn
from torch.nn.utils.rnn import pack_padded_sequence
class LSTM_att(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(LSTM_att, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.dropout = nn.Dropout(0.3)
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout = 0.4)
        self.fc = nn.Linear(hidden_size, output_size)
        self.concat_linear = nn.Linear(self.hidden_size * 2, self.hidden_size)

        
        self.attn = nn.Linear(self.hidden_size, hidden_size)
        self.attn1 = nn.Linear(self.hidden_size, 1)
    
        
    def forward(self, x, selected_k):
        #x, x_lengths, _ = batch
        #x_pack = x[~torch.any(x.isnan(),dim=1)]
        #x_pack = pack_padded_sequence(x, x_len, batch_first=True)
        #x = self.dropout(x_pack)
        lstm_out = torch.zeros(x.shape[0],selected_k)
        output = torch.zeros(x.shape[0],selected_k)
        for i in range(x.shape[0]):
            x_pack = x[i][~torch.any(x[i].isnan(),dim=1)]
        
            x2 = x_pack.to(torch.float32)
            x2, _ = self.lstm(x2)
        
           
            x3 = self.fc(x2[ -1, :])
            print(x3.shape)
            lstm_out[i] = x3
            x_hidden = x2
            x_out = x3
            attn_weights = self.attn1(x2) # (batch_size, seq_len, hidden_dim)
            attn_weights = torch.softmax(attn_weights, dim=1)
            attn_out = torch.sum(attn_weights * x2, dim=1)
            output[i] = self.fc(attn_out)
            
            
        return lstm_out, output
